"""
í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ìš”ì•½ ëª¨ë“ˆ
ê°œë³„ ë‰´ìŠ¤ ìš”ì•½ ê²°ê³¼ë¥¼ í‚¤ì›Œë“œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¶”ê°€ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
import json
import os
from typing import List, Dict
from tqdm import tqdm
from datetime import datetime
import glob
from collections import defaultdict


class KeywordSummarizer:
    """í‚¤ì›Œë“œë³„ ìš”ì•½ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = "gogamza/kobart-base-v2"):
        """
        Args:
            model_name: ì‚¬ìš©í•  KoBART ëª¨ë¸ëª…
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
        print("í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)
        
        print("KoBART ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = BartForConditionalGeneration.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def preprocess_text(self, text: str, max_length: int = 1024) -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            text: ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ ê¸¸ì´
            
        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì •ë¦¬
        text = text.strip()
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë¥´ê¸°
        if len(text) > max_length:
            text = text[:max_length]
        
        return text
    
    def summarize_text(self, text: str, max_length: int = 500, min_length: int = 100) -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
        
        Args:
            text: ìš”ì•½í•  í…ìŠ¤íŠ¸
            max_length: ìš”ì•½ ìµœëŒ€ ê¸¸ì´
            min_length: ìš”ì•½ ìµœì†Œ ê¸¸ì´
            
        Returns:
            ìš”ì•½ëœ í…ìŠ¤íŠ¸
        """
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_text = self.preprocess_text(text)
            
            if not processed_text:
                return ""
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(processed_text, return_tensors="pt", max_length=1024, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ìš”ì•½ ìƒì„±
            with torch.no_grad():
                summary_ids = self.model.generate(
                    inputs["input_ids"],
                    max_new_tokens=max_length,  # ìƒˆë¡œ ìƒì„±í•  í† í° ìˆ˜ ì œí•œ
                    min_new_tokens=min_length,  # ìµœì†Œ ìƒˆë¡œ ìƒì„±í•  í† í° ìˆ˜
                    num_beams=5,  # í’ˆì§ˆ ê°œì„ ì„ ìœ„í•´ 5ë¡œ ì¦ê°€
                    length_penalty=0.8,  # ì§§ì€ ìš”ì•½ì„ ìœ„í•´ ê°ì†Œ
                    early_stopping=True,
                    no_repeat_ngram_size=3,  # ë°˜ë³µ ë°©ì§€
                    do_sample=True,  # ë‹¤ì–‘ì„± ì¦ê°€
                    temperature=0.8,  # ì°½ì˜ì„± ì¡°ì ˆ
                    top_k=50,  # ìƒìœ„ í† í°ë§Œ ê³ ë ¤
                    top_p=0.9,  # ëˆ„ì  í™•ë¥  ì œí•œ
                    repetition_penalty=1.2  # ë°˜ë³µ ë°©ì§€ ê°•í™”
                )
            
            # ë””ì½”ë”©
            summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            
            return summary.strip()
            
        except Exception as e:
            print(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""
    
    def load_summarized_data(self, file_path: str) -> List[Dict]:
        """
        ìš”ì•½ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            file_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ìš”ì•½ëœ ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    
    def create_keyword_summary(self, processed_data: List[Dict], max_length: int = 500) -> Dict:
        """
        í‚¤ì›Œë“œë³„ë¡œ ë‰´ìŠ¤ë¥¼ ê·¸ë£¹í™”í•˜ê³  ì¶”ê°€ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            processed_data: ì²˜ë¦¬ëœ ë‰´ìŠ¤ ë°ì´í„°
            max_length: í‚¤ì›Œë“œë³„ ìš”ì•½ ìµœëŒ€ ê¸¸ì´
            
        Returns:
            í‚¤ì›Œë“œë³„ ìš”ì•½ ë”•ì…”ë„ˆë¦¬
        """
        # í‚¤ì›Œë“œë³„ë¡œ ë‰´ìŠ¤ ê·¸ë£¹í™”
        keyword_groups = defaultdict(list)
        for item in processed_data:
            keyword = item.get('keyword', '')
            if keyword and item.get('kobart_summary'):
                keyword_groups[keyword].append(item)
        
        print(f"\ní‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ê°œìˆ˜:")
        for keyword, items in keyword_groups.items():
            print(f"  {keyword}: {len(items)}ê°œ")
        
        # í‚¤ì›Œë“œë³„ ì¶”ê°€ ìš”ì•½ ìƒì„±
        keyword_summaries = {}
        
        for keyword, items in keyword_groups.items():
            print(f"\n=== {keyword} í‚¤ì›Œë“œ ì¶”ê°€ ìš”ì•½ ìƒì„± ì¤‘ ===")
            
            # í•´ë‹¹ í‚¤ì›Œë“œì˜ ëª¨ë“  ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©
            combined_summaries = []
            for item in items:
                summary = item.get('kobart_summary', '')
                title = item.get('title', '')
                if summary:
                    # ì œëª©ê³¼ ìš”ì•½ì„ ê²°í•©
                    combined_text = f"{title}: {summary}"
                    combined_summaries.append(combined_text)
            
            if combined_summaries:
                # ëª¨ë“  ìš”ì•½ì„ í•˜ë‚˜ì˜ ê¸´ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
                full_text = " ".join(combined_summaries)
                
                # 500ì ì •ë„ì˜ í‚¤ì›Œë“œë³„ ìš”ì•½ ìƒì„±
                keyword_summary = self.summarize_text(full_text, max_length, min_length=100)
                
                keyword_summaries[keyword] = {
                    'summary': keyword_summary,
                    'news_count': len(items),
                    'titles': [item.get('title', '') for item in items[:5]]  # ìƒìœ„ 5ê°œ ì œëª©
                }
                
                print(f"ìƒì„±ëœ ìš”ì•½ ({len(keyword_summary)}ì):")
                print(f"{keyword_summary[:200]}...")
            else:
                print(f"{keyword}: ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        return keyword_summaries


def save_keyword_summary_md(keyword_summaries: Dict, output_file: str):
    """í‚¤ì›Œë“œë³„ ìš”ì•½ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# ğŸ“Š í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ìš”ì•½\n\n")
        f.write(f"**ìƒì„±ì¼**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼')}\n\n")
        
        for keyword, summary_data in keyword_summaries.items():
            f.write(f"## ğŸ” {keyword} ({summary_data['news_count']}ê°œ ë‰´ìŠ¤)\n\n")
            f.write(f"### ğŸ“ ìš”ì•½\n")
            f.write(f"{summary_data['summary']}\n\n")
            
            f.write(f"### ğŸ“° ì£¼ìš” ë‰´ìŠ¤ ì œëª©\n")
            for i, title in enumerate(summary_data['titles'], 1):
                f.write(f"{i}. {title}\n")
            f.write("\n---\n\n")
    
    print(f"ë§ˆí¬ë‹¤ìš´ ìš”ì•½ì´ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def main():
    # ì„¤ì •
    max_length = 500  # í† í° ë‹¨ìœ„ë¡œ 500ê°œë¡œ ì„¤ì •
    min_length = 100  # ìµœì†Œ ê¸¸ì´
    
    # í‚¤ì›Œë“œ ìš”ì•½ê¸° ì´ˆê¸°í™”
    summarizer = KeywordSummarizer()
    
    # ìµœì‹  ìš”ì•½ëœ ë‰´ìŠ¤ íŒŒì¼ ì°¾ê¸° (processed í´ë”ì—ì„œ)
    news_files = glob.glob("data/processed/summarized_news_*.json")
    if not news_files:
        print("ì²˜ë¦¬í•  ìš”ì•½ëœ ë‰´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € python src/02_kobart_summarize.py ë¥¼ ì‹¤í–‰í•˜ì—¬ ê°œë³„ ë‰´ìŠ¤ ìš”ì•½ì„ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    latest_file = max(news_files, key=os.path.getctime)
    print(f"ì²˜ë¦¬í•  íŒŒì¼: {latest_file}")
    
    # ìš”ì•½ëœ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
    news_data = summarizer.load_summarized_data(latest_file)
    print(f"ë¡œë“œëœ ë‰´ìŠ¤: {len(news_data)}ê°œ")
    
    # í‚¤ì›Œë“œë³„ ìš”ì•½ ìƒì„±
    print(f"\n=== í‚¤ì›Œë“œë³„ ìš”ì•½ ìƒì„± ì‹œì‘ ===")
    print(f"í‚¤ì›Œë“œë³„ ìš”ì•½ ê¸¸ì´: {max_length} í† í°")
    keyword_summaries = summarizer.create_keyword_summary(news_data, max_length)
    
    # í‚¤ì›Œë“œë³„ ìš”ì•½ ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d")
    keyword_output_file = f"data/processed/keyword_summaries_{timestamp}.json"
    with open(keyword_output_file, 'w', encoding='utf-8') as f:
        json.dump(keyword_summaries, f, ensure_ascii=False, indent=2)
    
    print(f"í‚¤ì›Œë“œë³„ ìš”ì•½ ì €ì¥: {keyword_output_file}")
    
    # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œë„ ì €ì¥
    md_output_file = f"data/processed/keyword_summaries_{timestamp}.md"
    save_keyword_summary_md(keyword_summaries, md_output_file)
    
    print(f"ë§ˆí¬ë‹¤ìš´ ìš”ì•½ ì €ì¥: {md_output_file}")
    
    print(f"\n=== í‚¤ì›Œë“œë³„ ìš”ì•½ ì™„ë£Œ ===")
    print(f"í‚¤ì›Œë“œë³„ ìš”ì•½: {len(keyword_summaries)}ê°œ")


if __name__ == "__main__":
    main() 