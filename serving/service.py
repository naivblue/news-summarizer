"""
BentoML ë‰´ìŠ¤ ìš”ì•½ ì„œë¹„ìŠ¤ (ê°„ë‹¨í•œ í•¨ìˆ˜ ê¸°ë°˜)
"""

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
import sys, os
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, "src"))

import bentoml
from typing import Dict, List, Any
import json
from datetime import datetime
import importlib

# ì •ì  ëª¨ë“ˆ import (ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ëª… ì²˜ë¦¬)
news_crawl = importlib.import_module("01_news_crawl")
kobart_summarize = importlib.import_module("02_kobart_summarize")
keyword_summarize = importlib.import_module("03_keyword_summarize")

# í´ë˜ìŠ¤ ì¶”ì¶œ
NewsCrawler = news_crawl.NewsCrawler
KoBARTSummarizer = kobart_summarize.KoBARTSummarizer
KeywordSummarizer = keyword_summarize.KeywordSummarizer

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
print("ğŸš€ ë‰´ìŠ¤ ìš”ì•½ ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘...")

# í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
crawler = NewsCrawler()
print("âœ… ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")

# KoBART ìš”ì•½ê¸° ì´ˆê¸°í™”
summarizer = KoBARTSummarizer()
print("âœ… KoBART ìš”ì•½ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

# í‚¤ì›Œë“œ ìš”ì•½ê¸° ì´ˆê¸°í™”
keyword_summarizer = KeywordSummarizer()
print("âœ… í‚¤ì›Œë“œ ìš”ì•½ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

print("ğŸ‰ ëª¨ë“  ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")


# BentoML ì„œë¹„ìŠ¤ ì •ì˜
@bentoml.service
class SimpleNewsService:
    
    @bentoml.api
    def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ ì²´í¬ API"""
        return {
            "status": "healthy",
            "service": "news_summarizer",
            "timestamp": datetime.now().isoformat(),
            "models": {
                "crawler": "ready",
                "summarizer": "ready",
                "keyword_summarizer": "ready"
            }
        }
    

    @bentoml.api
    def crawl_news(self, keywords: List[str] = ["ETF"], pages: int = 3) -> Dict[str, Any]:
        """ë‰´ìŠ¤ í¬ë¡¤ë§ API"""
        try:
            print("ğŸ” ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
            all_news = []
            for keyword in keywords:
                news_data = crawler.crawl_naver_news(keyword, max_pages=pages)
                all_news.extend(news_data)
            
            return {
                "status": "success",
                "data": {
                    "news_count": len(all_news),
                    "news_data": all_news[:10],  # ì²˜ìŒ 10ê°œë§Œ ë°˜í™˜
                    "keywords": keywords
                },
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "status": "error",
                "message": str(e),
                "timestamp": datetime.now().isoformat()
            }

    @bentoml.api
    def summarize_news(self, news_data: List[Dict]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ìš”ì•½ API"""
        try:
            print("ğŸ“ ë‰´ìŠ¤ ìš”ì•½ ì‹œì‘...")
            summarized_data = summarizer.process_news_data(
                news_data,
                max_length=128,
                min_length=10,
                batch_size=4
            )
            
            return {
                "status": "success",
                "data": {
                    "original_count": len(news_data),
                    "summarized_count": len(summarized_data),
                    "summarized_data": summarized_data
                },
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "status": "error",
                "message": str(e),
                "timestamp": datetime.now().isoformat()
            }

    @bentoml.api
    def full_pipeline(self, keywords: List[str] = ["ETF"], pages: int = 3) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ API"""
        try:
            print("ğŸ” ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
            all_news = []
            for keyword in keywords:
                news_data = crawler.crawl_naver_news(keyword, max_pages=pages)
                all_news.extend(news_data)
            
            if not all_news:
                return {
                    "status": "error",
                    "message": "í¬ë¡¤ë§ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "timestamp": datetime.now().isoformat()
                }
            
            print("ğŸ“ ë‰´ìŠ¤ ìš”ì•½ ì‹œì‘...")
            summarized_data = summarizer.process_news_data(
                all_news[:10],  # ì²˜ìŒ 10ê°œë§Œ ìš”ì•½
                max_length=128,
                min_length=10,
                batch_size=4
            )
            
            print("ğŸ”‘ í‚¤ì›Œë“œë³„ ìš”ì•½ ì‹œì‘...")
            keyword_summaries = keyword_summarizer.create_keyword_summary(
                summarized_data,
                max_length=300
            )
            
            return {
                "status": "success",
                "data": {
                    "crawl_count": len(all_news),
                    "summarized_count": len(summarized_data),
                    "keyword_summaries": keyword_summaries,
                    "stats": {
                        "total_news": len(all_news),
                        "summarized_news": len(summarized_data),
                        "keywords": list(keyword_summaries.keys()) if keyword_summaries else []
                    }
                },
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "status": "error",
                "message": str(e),
                "timestamp": datetime.now().isoformat()
            } 